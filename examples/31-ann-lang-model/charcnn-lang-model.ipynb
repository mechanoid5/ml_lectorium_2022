{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9095d55d",
   "metadata": {},
   "source": [
    "__Нейросетевая языковая модель на основе CharCNN+LSTM__ \n",
    "\n",
    "Евгений Борисов <esborisov@sevsu.ru>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a337c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82d840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601744\n"
     ]
    }
   ],
   "source": [
    "# загружаем текст\n",
    "import gzip\n",
    "# with gzip.open('../data/dostoevsky-besy-p2.txt.gz','rt',encoding='utf-8') as f: data = f.read()     \n",
    "with gzip.open('../data/lobas-taxisty.txt.gz','rt',encoding='utf-8') as f: data = f.read()     \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a1033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Желтые короли. Записки нью-йоркского таксиста.\n",
      "\n",
      "В.Лобас\n",
      "\n",
      "\n",
      "\n",
      "Товарищам моим - белым и черным, американцам и эмигрантам из России и Израиля, из Греции и Кореи, арабам, китайцам, полякам и всем прочим таксистам города Нью-Йорк в знак глубокого уважения к их нечеловеческому труду эту горькую книгу посвящаю...\n",
      "\n",
      "Водитель No 320718\n",
      "\n",
      "\n",
      "\"Я никогда не знал бы многое из того что я знаю, и половины чего достаточно, чтобы отравить навсегда несколько человеческих жизней, если бы мне не пришлось сделаться шофером такси...\" \n"
     ]
    }
   ],
   "source": [
    "print(data[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5574b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[:1024*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc375a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize as nltk_sentence_split\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenize_word\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    \n",
    "    def __init__(self,text,language='russian'):\n",
    "        self._word2index = dict()\n",
    "        self._index2word = dict()\n",
    "        \n",
    "        self._max_word_len = 0\n",
    "        self._max_sentence_len = 0\n",
    "        \n",
    "        self._data = self._tokenize(text)\n",
    "        self._check_len()\n",
    "        self._build_vocab()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _tokenize(text):\n",
    "        return [ # выполняем токенизацию\n",
    "            nltk_tokenize_word(s,language=language) # разбиваем предложения на слова\n",
    "            for s in nltk_sentence_split(text,language=language) # режем текст на отдельные предложения\n",
    "        ]\n",
    "    \n",
    "    def _check_len(self):\n",
    "        self._max_word_len = max([ len(t) for t in set(itertools.chain(*self._data)) ])\n",
    "        self._max_sentence_len = max([ len(s) for s in self._data ])\n",
    "        return self\n",
    "    \n",
    "    def _build_vocab(self): # формируем словарь \n",
    "        # список токенов за исключением специальных\n",
    "        words = sorted( set(itertools.chain(*self._data)) ) \n",
    "        # нумеруем токены\n",
    "        self._word2index = { w:i for i,w in enumerate(words) }\n",
    "        # обратное преобразование из номера в токен\n",
    "        self._index2word = { i:w for  w,i in self._word2index.items() }\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCharData(TextData):\n",
    "    \n",
    "    def __init__(self,text,language='russian'):\n",
    "        super().__init__(text,language)\n",
    "        self._abc2index = dict()\n",
    "        self._index2abc = dict()\n",
    "        self._build_abc()\n",
    "        \n",
    "    def _build_abc(self): # формируем словарь \n",
    "        abc = sorted(set(' '.join(set(itertools.chain(*self._data)))))\n",
    "        self._abc2index  = { w:i for i,w in enumerate(abc) }\n",
    "        self._index2abc = { i:w for  w,i in self._abc2index.items() }\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c038d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequencer:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f643b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse\n",
    "# sentence = text[6]\n",
    "# len(sentence)*len(abc2index)\n",
    "# max_sentence_len*len(abc2index)\n",
    "# abc2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a52ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9e6c75",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abc2index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoken2map\u001b[39m(token,abc2index\u001b[38;5;241m=\u001b[39m\u001b[43mabc2index\u001b[49m,max_word_len\u001b[38;5;241m=\u001b[39mmax_word_len):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix( \n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mtuple\u001b[39m([ \n\u001b[1;32m      7\u001b[0m             (\u001b[38;5;241m1\u001b[39m,)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(token), \u001b[38;5;66;03m# индикатор (значение в ячейке матрицы)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         shape\u001b[38;5;241m=\u001b[39m[max_word_len,\u001b[38;5;28mlen\u001b[39m(abc2index)],\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m token2map(token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mдополняем\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'abc2index' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "def token2map(token,abc2index=abc2index,max_word_len=max_word_len):\n",
    "    return sparse.csr_matrix( \n",
    "        tuple([ \n",
    "            (1,)*len(token), # индикатор (значение в ячейке матрицы)\n",
    "            (   # индексы матрицы\n",
    "                tuple(range(len(token))), # позиция символа в слове\n",
    "                tuple([ abc2index[c] for c in token ]), # номер символа в словаре\n",
    "            ) \n",
    "        ]), \n",
    "        shape=[max_word_len,len(abc2index)],\n",
    "    )\n",
    "\n",
    "token2map(token='дополняем').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3670865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2map(\n",
    "        sentence,\n",
    "        abcabc2index=abc2index,\n",
    "        max_word_len=max_word_len,\n",
    "        max_sentence_len=max_sentence_len,\n",
    "    ):\n",
    "        return [ # дополняем до максиамального размера предложения нулями\n",
    "            sparse.csr_matrix(np.zeros( ( max_word_len,len(abc2index) ) ) )  \n",
    "            for _ in range(max_sentence_len-len(sentence)) \n",
    "        ] + [ # кодируем предложение\n",
    "            token2map(token,abc2index=abc2index,max_word_len=max_word_len) # строим карту токена\n",
    "            for token in sentence \n",
    "        ]\n",
    "\n",
    "# len( sentence2map(sentence) )\n",
    "# 198,(25,139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c44fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = text[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab083038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = np.array([\n",
    "#     [ word2index[token] for token in sentence[i:i+seq_len] ] # собираем токены и их левые контексты\n",
    "#     for sentence in text if len(sentence)>seq_len # все предложения, которые длиннее размера контекста  \n",
    "#     for i in range(len(sentence)-seq_len) # двигаем окно размера seq_len по предложению\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f60539f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41529, 16), dtype('int64'), (41529, 24319), dtype('float32'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # собираем датасет [ левый контекст, слово ]\n",
    "# import numpy as np\n",
    "# import keras.utils as ku \n",
    "\n",
    "# seq_len = 16+1 # размер левого контекста + 1 слово\n",
    "\n",
    "# sequences = np.array([\n",
    "#     [ word2index[token] for token in sentence[i:i+seq_len] ] # собираем токены и их левые контексты\n",
    "#     for sentence in text if len(sentence)>seq_len # все предложения, которые длиннее размера контекста  \n",
    "#     for i in range(len(sentence)-seq_len) # двигаем окно размера seq_len по предложению\n",
    "# ])\n",
    "\n",
    "# # sequences.shape\n",
    "\n",
    "# inputs = sequences[:,:-1] # контексты\n",
    "# targets = sequences[:,-1] # целевые токены для контекстов \n",
    "\n",
    "# del sequences\n",
    "\n",
    "# # выполняем OHE, конвертируем номера токенов targets в векторы {0,1}\n",
    "# targets = ku.np_utils.to_categorical(targets, num_classes=total_words)\n",
    "\n",
    "# inputs.shape, inputs.dtype, targets.shape, targets.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2map(\n",
    "        sentence,\n",
    "        abc_len=len(abc2index),\n",
    "        max_word_len=max_word_len,\n",
    "        max_sentence_len=max_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c77362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(inputs,targets,,batch_size=32):\n",
    "    for i in range(0,len(text),batch_size):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa042199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем языковую модель\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 128, input_length=inputs.shape[1]))\n",
    "model.add(LSTM(1024))\n",
    "# model.add(LSTM(128,return_sequences=True))\n",
    "# model.add(LSTM(128))\n",
    "model.add(Dense(targets.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503780da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # описание нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# обучем модель по левому контексту предсказывать следующий за ним токен\n",
    "history = model.fit( \n",
    "    inputs,\n",
    "    targets, \n",
    "    epochs=10, \n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# изменение значений ф-ции потери и погрешности в процессе обучения модели\n",
    "\n",
    "loss = { k:history.history[k] for k in history.history if k.find('loss')>-1 }  \n",
    "accuracy = { k:history.history[k] for k in history.history if k.find('acc')>-1 }  \n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(16,3))\n",
    "\n",
    "for k in loss: \n",
    "    ax1.plot(loss[k], label=k)\n",
    "    ax1.legend()    \n",
    "    ax1.grid()\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    \n",
    "for k in accuracy: \n",
    "    ax2.plot(accuracy[k], label=k)\n",
    "    ax2.legend()    \n",
    "    ax2.grid()\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тестируем модель\n",
    "\n",
    "from numpy import random as rng\n",
    "from random import sample\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# выбираем случайный контекст из исходных данных\n",
    "def get_sample(text,seq_len=seq_len-2,max_seq_len=seq_len-1):\n",
    "    assert seq_len<max_seq_len\n",
    "    for _ in range(100):\n",
    "        sentence = sample(text,1)[0]\n",
    "        if len(sentence)>seq_len*2:\n",
    "            i = rng.randint(len(sentence)-seq_len*2)\n",
    "            return sentence[i:i+seq_len]\n",
    "    return [PAD,]\n",
    "\n",
    "# кодируем контекст\n",
    "def encode_sentence(sentence,word2index=word2index,max_seq_len=seq_len-1):\n",
    "    return np.array(\n",
    "            pad_sequences(\n",
    "                [[ word2index[token] for token in sentence ],], \n",
    "                maxlen=max_seq_len, \n",
    "                padding='pre',\n",
    "            )\n",
    "        )\n",
    "\n",
    "# encode_sentence(get_sample(text)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be12abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for __ in range(10):\n",
    "\n",
    "    sentence0 = get_sample(text) # выбираем контекст\n",
    "    answer = [] # предсказанные токены\n",
    "    sentence = [PAD]+sentence0 # начальный контекст\n",
    "\n",
    "    for _ in range(27):\n",
    "        sequence = encode_sentence(sentence[1:]) # кодируем контекст\n",
    "        predicted_index = np.argmax( model.predict(sequence) ) # продолжение контекста то модели \n",
    "        predicted_word = index2word[predicted_index] # декодируем ответ модели\n",
    "        answer.append(predicted_word) # запоминаем токен предсказанный моделью\n",
    "        if predicted_index == word2index[EOS]: break # если предсказан конец предложения то завершаем\n",
    "        sentence.append(predicted_word) # предсказанный токен в контекст\n",
    "        \n",
    "    print('\\n'+'init> '+' '.join(sentence0)+'\\n'+'lstm> ' + ' '.join(answer) )\n",
    "    print(' - '*30 )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e519e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145351bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # выполняем токенизацию\n",
    "# import itertools\n",
    "# from nltk.tokenize import sent_tokenize as nltk_sentence_split\n",
    "# from nltk.tokenize import word_tokenize as nltk_tokenize_word\n",
    "\n",
    "# text = [ \n",
    "#     nltk_tokenize_word(s,language='russian') # разбиваем предложения на слова\n",
    "#     for s in nltk_sentence_split(data,language='russian') # режем текст на отдельные предложения\n",
    "# ]\n",
    "\n",
    "# # del data\n",
    "\n",
    "# max_word_len = max([ len(t) for t in set(itertools.chain(*text)) ])\n",
    "# max_sentence_len = max([ len(s) for s in text ])\n",
    "\n",
    "# len(text), max_sentence_len, max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a6b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc = sorted(set(' '.join(set(itertools.chain(*text)))))\n",
    "# # abc =  sorted( set(data) - set(['\\n',]) )\n",
    "# abc2index  = { w:i for i,w in enumerate(abc) }\n",
    "# index2abc = { i:w for  w,i in abc2index.items() }\n",
    "# del abc\n",
    "\n",
    "# len(abc2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853bf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # формируем словарь \n",
    "\n",
    "# # список токенов за исключением специальных\n",
    "# words = sorted( set(itertools.chain(*text)) ) \n",
    "\n",
    "# # нумеруем токены\n",
    "# word2index = { w:i for i,w in enumerate(words) }\n",
    "# del words\n",
    "\n",
    "# # обратное преобразование из номера в токен\n",
    "# index2word = { i:w for  w,i in word2index.items() }\n",
    "\n",
    "# total_words = len(word2index) # всего токенов в словаре\n",
    "\n",
    "# total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44c26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7bd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse\n",
    "\n",
    "# sentence = text[6]\n",
    "\n",
    "# len(sentence)*len(abc2index)\n",
    "\n",
    "# max_sentence_len*len(abc2index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
